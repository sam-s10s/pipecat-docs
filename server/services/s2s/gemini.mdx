---
title: "Gemini Multimodal Live"
description: "A real-time, multimodal conversational AI service powered by Google’s Gemini"
---

The `GeminiMultimodalLiveLLMService` enables natural, real-time conversations with Google’s Gemini model. It provides built-in audio transcription, voice activity detection, and context management for creating interactive AI experiences. It provides:

<CardGroup cols={2}>
  <Card title="Real-time Interaction" icon="video">
    Stream audio and video in real-time with low latency response times
  </Card>

<Card title="Speech Processing" icon="waveform-lines">
  Built-in speech-to-text and text-to-speech capabilities with multiple voice
  options
</Card>

<Card title="Voice Activity Detection" icon="microphone">
  Automatic detection of speech start/stop for natural conversations
</Card>

  <Card title="Context Management" icon="brain">
    Intelligent handling of conversation history and system instructions
  </Card>
</CardGroup>

<Tip>
  Want to start building? Check out our [Gemini Multimodal Live
  Guide](/guides/features/gemini-multimodal-live).
</Tip>

## Installation

To use `GeminiMultimodalLiveLLMService`, install the required dependencies:

```bash
pip install "pipecat-ai[google]"
```

You’ll need to set up your Google API key as an environment variable: `GOOGLE_API_KEY`.

## Basic Usage

Here’s a simple example of setting up a conversational AI bot with Gemini Multimodal Live:

```python
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiMultimodalLiveLLMService,
    InputParams,
    GeminiMultimodalModalities
)

llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    voice_id="Aoede",                                # Voices: Aoede, Charon, Fenrir, Kore, Puck
    params=InputParams(
        temperature=0.7,                             # Set model input params
        language=Language.EN_US,                     # Set language (30+ languages supported)
        modalities=GeminiMultimodalModalities.AUDIO  # Response modality
    )
)
```

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your Google API key
</ParamField>

<ParamField
  path="base_url"
  type="str"
  default="generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent"
>
  API endpoint URL
</ParamField>

<ParamField
  path="model"
  type="str"
  default="models/gemini-2.5-flash-preview-native-audio-dialog"
>
  Gemini model to use (upgraded to new v1beta model)
</ParamField>

<ParamField path="voice_id" type="str" default="Charon">
  Voice for text-to-speech (options: Aoede, Charon, Fenrir, Kore, Puck)
</ParamField>

```python
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    voice_id="Puck",  # Choose your preferred voice
)
```

<ParamField path="system_instruction" type="str" optional>
  High-level instructions that guide the model's behavior
</ParamField>

```python
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    system_instruction="Talk like a pirate.",
)
```

<ParamField path="start_audio_paused" type="bool" default="False">
  Whether to start with audio input paused
</ParamField>

<ParamField path="start_video_paused" type="bool" default="False">
  Whether to start with video input paused
</ParamField>

<ParamField path="tools" type="Union[List[dict], ToolsSchema]" optional>
  Tools/functions available to the model
</ParamField>

<ParamField
  path="inference_on_context_initialization"
  type="bool"
  default="True"
>
  Whether to generate a response when context is first set
</ParamField>

### Input Parameters

<ParamField path="frequency_penalty" type="float" optional default="None">
  Penalizes repeated tokens. Range: 0.0 to 2.0
</ParamField>

<ParamField path="max_tokens" type="int" optional default="4096">
  Maximum number of tokens to generate
</ParamField>

<ParamField
  path="modalities"
  type="GeminiMultimodalModalities"
  optional
  default="AUDIO"
>
  Response modalities to include (options: `AUDIO`, `TEXT`).
</ParamField>

<ParamField path="presence_penalty" type="float" optional default="None">
  Penalizes tokens based on their presence in the text. Range: 0.0 to 2.0
</ParamField>

<ParamField path="temperature" type="float" optional default="None">
  Controls randomness in responses. Range: 0.0 to 2.0
</ParamField>

<ParamField path="language" type="Language" optional default="Language.EN_US">
  Language for generation. Over 30 languages are supported.
</ParamField>

<ParamField
  path="media_resolution"
  type="GeminiMediaResolution"
  optional
  default="UNSPECIFIED"
>
  
Controls image processing quality and token usage:

- `LOW`: Uses 64 tokens
- `MEDIUM`: Uses 256 tokens
- `HIGH`: Zoomed reframing with 256 tokens

</ParamField>

<ParamField path="vad" type="GeminiVADParams" optional>

Voice Activity Detection configuration:

- `disabled`: Toggle VAD on/off
- `start_sensitivity`: How quickly speech is detected (HIGH/LOW)
- `end_sensitivity`: How quickly turns end after pauses (HIGH/LOW)
- `prefix_padding_ms`: Milliseconds of audio to keep before speech
- `silence_duration_ms`: Milliseconds of silence to end a turn

</ParamField>

```python
from pipecat.services.gemini_multimodal_live.events import (
    StartSensitivity,
    EndSensitivity
)
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiVADParams,
    GeminiMediaResolution,
)

llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    params=InputParams(
        temperature=0.7,
        language=Language.ES,                         # Spanish language
        media_resolution=GeminiMediaResolution.HIGH,  # Higher quality image processing
        vad=GeminiVADParams(
            start_sensitivity=StartSensitivity.HIGH,  # Detect speech quickly
            end_sensitivity=EndSensitivity.LOW,       # Allow longer pauses
            prefix_padding_ms=300,                    # Keep 300ms before speech
            silence_duration_ms=1000,                 # End turn after 1s silence
        )
    )
)
```

<ParamField path="top_k" type="int" optional default="None">
  Limits vocabulary to k most likely tokens. Minimum: 0
</ParamField>

<ParamField path="top_p" type="float" optional default="None">
  Cumulative probability cutoff for token selection. Range: 0.0 to 1.0
</ParamField>

<ParamField
  path="context_window_compression"
  type="ContextWindowCompressionParams"
  optional
>
  Parameters for managing the context window: - `enabled`: Enable/disable
  compression (default: False) - `trigger_tokens`: Number of tokens that trigger
  compression (default: None, uses 80% of context window)
</ParamField>

```python
from pipecat.services.gemini_multimodal_live.gemini import (
    ContextWindowCompressionParams
)

llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    params=InputParams(
        top_p=0.9,               # More focused token selection
        top_k=40,                # Limit vocabulary options
        context_window_compression=ContextWindowCompressionParams(
            enabled=True,
            trigger_tokens=8000  # Compress when reaching 8000 tokens
        )
    )
)
```

## Methods

<ParamField path="set_audio_input_paused(paused: bool)" type="method">
  Pause or unpause audio input processing
</ParamField>

<ParamField path="set_video_input_paused(paused: bool)" type="method">
  Pause or unpause video input processing
</ParamField>

<ParamField
  path="set_model_modalities(modalities: GeminiMultimodalModalities)"
  type="method"
>
  Change the response modality (TEXT or AUDIO)
</ParamField>

<ParamField path="set_language(language: Language)" type="method">
  Change the language for generation
</ParamField>

<ParamField path="set_context(context: OpenAILLMContext)" type="method">
  Set the conversation context explicitly
</ParamField>

<ParamField
  path="create_context_aggregator(context: OpenAILLMContext, user_params: LLMUserAggregatorParams, assistant_params: LLMAssistantAggregatorParams)"
  type="method"
>
  Create context aggregators for managing conversation state
</ParamField>

## Frame Types

### Input Frames

<ParamField path="InputAudioRawFrame" type="Frame">
  Raw audio data for speech input
</ParamField>

<ParamField path="InputImageRawFrame" type="Frame">
  Raw image data for visual input
</ParamField>

<ParamField path="StartInterruptionFrame" type="Frame">
  Signals start of user interruption
</ParamField>

<ParamField path="UserStartedSpeakingFrame" type="Frame">
  Signals user started speaking
</ParamField>

<ParamField path="UserStoppedSpeakingFrame" type="Frame">
  Signals user stopped speaking
</ParamField>

<ParamField path="OpenAILLMContextFrame" type="Frame">
  Contains conversation context
</ParamField>

<ParamField path="LLMMessagesAppendFrame" type="Frame">
  Adds messages to the conversation
</ParamField>

<ParamField path="LLMUpdateSettingsFrame" type="Frame">
  Updates LLM settings
</ParamField>

<ParamField path="LLMSetToolsFrame" type="Frame">
  Sets available tools for the LLM
</ParamField>

### Output Frames

<ParamField path="TTSAudioRawFrame" type="Frame">
  Generated speech audio
</ParamField>

<ParamField path="TTSStartedFrame" type="Frame">
  Signals start of speech synthesis
</ParamField>

<ParamField path="TTSStoppedFrame" type="Frame">
  Signals end of speech synthesis
</ParamField>

<ParamField path="LLMTextFrame" type="Frame">
  Generated text responses from the LLM
</ParamField>

<ParamField path="TTSTextFrame" type="Frame">
  Text used for speech synthesis
</ParamField>

<ParamField path="TranscriptionFrame" type="Frame">
  Speech transcriptions from user audio
</ParamField>

<ParamField path="LLMFullResponseStartFrame" type="Frame">
  Signals the start of a complete LLM response
</ParamField>

<ParamField path="LLMFullResponseEndFrame" type="Frame">
  Signals the end of a complete LLM response
</ParamField>

## Function Calling

This service supports function calling (also known as tool calling) which allows the LLM to request information from external services and APIs. For example, you can enable your bot to:

- Check current weather conditions
- Query databases
- Access external APIs
- Perform custom actions

See the [Function Calling guide](/learn/function-calling) for:

- Detailed implementation instructions
- Provider-specific function definitions
- Handler registration examples
- Control over function call behavior
- Complete usage examples

## Token Usage Tracking

Gemini Multimodal Live automatically tracks token usage metrics, providing:

- Prompt token counts
- Completion token counts
- Total token counts
- Detailed token breakdowns by modality (text, audio)

These metrics can be used for monitoring usage, optimizing costs, and understanding model performance.

## Language Support

Gemini Multimodal Live supports the following languages:

| Language Code     | Description          | Gemini Code |
| ----------------- | -------------------- | ----------- |
| `Language.AR`     | Arabic               | `ar-XA`     |
| `Language.BN_IN`  | Bengali (India)      | `bn-IN`     |
| `Language.CMN_CN` | Chinese (Mandarin)   | `cmn-CN`    |
| `Language.DE_DE`  | German (Germany)     | `de-DE`     |
| `Language.EN_US`  | English (US)         | `en-US`     |
| `Language.EN_AU`  | English (Australia)  | `en-AU`     |
| `Language.EN_GB`  | English (UK)         | `en-GB`     |
| `Language.EN_IN`  | English (India)      | `en-IN`     |
| `Language.ES_ES`  | Spanish (Spain)      | `es-ES`     |
| `Language.ES_US`  | Spanish (US)         | `es-US`     |
| `Language.FR_FR`  | French (France)      | `fr-FR`     |
| `Language.FR_CA`  | French (Canada)      | `fr-CA`     |
| `Language.GU_IN`  | Gujarati (India)     | `gu-IN`     |
| `Language.HI_IN`  | Hindi (India)        | `hi-IN`     |
| `Language.ID_ID`  | Indonesian           | `id-ID`     |
| `Language.IT_IT`  | Italian (Italy)      | `it-IT`     |
| `Language.JA_JP`  | Japanese (Japan)     | `ja-JP`     |
| `Language.KN_IN`  | Kannada (India)      | `kn-IN`     |
| `Language.KO_KR`  | Korean (Korea)       | `ko-KR`     |
| `Language.ML_IN`  | Malayalam (India)    | `ml-IN`     |
| `Language.MR_IN`  | Marathi (India)      | `mr-IN`     |
| `Language.NL_NL`  | Dutch (Netherlands)  | `nl-NL`     |
| `Language.PL_PL`  | Polish (Poland)      | `pl-PL`     |
| `Language.PT_BR`  | Portuguese (Brazil)  | `pt-BR`     |
| `Language.RU_RU`  | Russian (Russia)     | `ru-RU`     |
| `Language.TA_IN`  | Tamil (India)        | `ta-IN`     |
| `Language.TE_IN`  | Telugu (India)       | `te-IN`     |
| `Language.TH_TH`  | Thai (Thailand)      | `th-TH`     |
| `Language.TR_TR`  | Turkish (Turkey)     | `tr-TR`     |
| `Language.VI_VN`  | Vietnamese (Vietnam) | `vi-VN`     |

You can set the language using the `language` parameter:

```python
from pipecat.transcriptions.language import Language
from pipecat.services.gemini_multimodal_live.gemini import (
    GeminiMultimodalLiveLLMService,
    InputParams
)

# Set language during initialization
llm = GeminiMultimodalLiveLLMService(
    api_key=os.getenv("GOOGLE_API_KEY"),
    params=InputParams(language=Language.ES_ES)  # Spanish (Spain)
)
```

## Next Steps

### Examples

- [Foundational Example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/26a-gemini-multimodal-live-transcription.py)
  Basic implementation showing core features and transcription

- [Simple Chatbot](https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot)
  A client/server example showing how to build a Pipecat JS or React client that connects to a Gemini Live Pipecat bot.

### Learn More

Check out our [Gemini Multimodal Live Guide](/guides/features/gemini-multimodal-live) for detailed explanations and best practices.
