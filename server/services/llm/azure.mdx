---
title: "Azure"
description: "Large Language Model service implementation using Azure OpenAI API"
---

## Overview

`AzureLLMService` provides access to Azure OpenAI's language models through an OpenAI-compatible interface. It inherits from `OpenAILLMService` and supports streaming responses, function calling, and context management.

<CardGroup cols={3}>
  <Card
    title="API Reference"
    icon="code"
    href="https://reference-server.pipecat.ai/en/latest/api/pipecat.services.azure.llm.html"
  >
    Complete API documentation and method details
  </Card>
  <Card
    title="Azure OpenAI Docs"
    icon="book"
    href="https://learn.microsoft.com/en-us/azure/ai-services/openai/"
  >
    Official Azure OpenAI documentation and setup
  </Card>
  <Card
    title="Example Code"
    icon="play"
    href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/14h-function-calling-azure.py"
  >
    Working example with function calling
  </Card>
</CardGroup>

## Installation

To use Azure OpenAI services, install the required dependency:

```bash
pip install "pipecat-ai[azure]"
```

You'll need to set up your Azure OpenAI credentials:

- `AZURE_CHATGPT_API_KEY` - Your Azure OpenAI API key
- `AZURE_CHATGPT_ENDPOINT` - Your Azure OpenAI endpoint URL
- `AZURE_CHATGPT_MODEL` - Your model deployment name

<Tip>
  Get your credentials from the [Azure Portal](https://portal.azure.com/) under
  your Azure OpenAI resource.
</Tip>

## Frames

### Input

- `OpenAILLMContextFrame` - Conversation context and history
- `LLMMessagesFrame` - Direct message list
- `VisionImageRawFrame` - Images for vision processing
- `LLMUpdateSettingsFrame` - Runtime parameter updates

### Output

- `LLMFullResponseStartFrame` / `LLMFullResponseEndFrame` - Response boundaries
- `LLMTextFrame` - Streamed completion chunks
- `FunctionCallInProgressFrame` / `FunctionCallResultFrame` - Function call lifecycle
- `ErrorFrame` - API or processing errors

## Azure vs OpenAI Differences

| Feature            | Azure OpenAI              | Standard OpenAI      |
| ------------------ | ------------------------- | -------------------- |
| **Authentication** | API key + endpoint        | API key only         |
| **Deployment**     | Custom deployment names   | Model names directly |
| **Compliance**     | Enterprise SOC, HIPAA     | Standard compliance  |
| **Regional**       | Multiple Azure regions    | OpenAI regions only  |
| **Pricing**        | Azure billing integration | OpenAI billing       |

## Function Calling

<Card
  title="Function Calling Guide"
  icon="function"
  href="/learn/function-calling"
>
  Learn how to implement function calling with standardized schemas, register
  handlers, manage context properly, and control execution flow in your
  conversational AI applications.
</Card>

## Context Management

<Card
  title="Context Management Guide"
  icon="messages"
  href="/learn/context-management"
>
  Learn how to manage conversation context, handle message history, and
  integrate context aggregators for consistent conversational experiences.
</Card>

## Usage Example

```python
import os
from pipecat.services.azure.llm import AzureLLMService
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema

# Configure Azure OpenAI service
llm = AzureLLMService(
    api_key=os.getenv("AZURE_CHATGPT_API_KEY"),
    endpoint=os.getenv("AZURE_CHATGPT_ENDPOINT"),
    model=os.getenv("AZURE_CHATGPT_MODEL"),  # Your deployment name
    params=AzureLLMService.InputParams(
        temperature=0.7,
        max_completion_tokens=1000
    )
)

# Define function for tool calling
weather_function = FunctionSchema(
    name="get_current_weather",
    description="Get current weather information",
    properties={
        "location": {
            "type": "string",
            "description": "City and state, e.g. San Francisco, CA"
        },
        "format": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "Temperature unit to use"
        }
    },
    required=["location", "format"]
)

tools = ToolsSchema(standard_tools=[weather_function])

# Create context
context = OpenAILLMContext(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant. Keep responses concise for voice output."
        }
    ],
    tools=tools
)

# Create context aggregators
context_aggregator = llm.create_context_aggregator(context)

# Register function handler with event callback
async def fetch_weather(params):
    location = params.arguments["location"]
    await params.result_callback({"conditions": "sunny", "temperature": "75Â°F"})

llm.register_function("get_current_weather", fetch_weather)

# Optional: Add function call feedback
@llm.event_handler("on_function_calls_started")
async def on_function_calls_started(service, function_calls):
    await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    transport.output(),
    context_aggregator.assistant()
])
```

## Metrics

Inherits all OpenAI metrics capabilities:

- **Time to First Byte (TTFB)** - Response latency measurement
- **Processing Duration** - Total request processing time
- **Token Usage** - Prompt tokens, completion tokens, and totals

<Info>
  [Learn how to enable Metrics](/guides/fundamentals/metrics) in your Pipeline.
</Info>

## Additional Notes

- **OpenAI Compatibility**: Full compatibility with OpenAI API features and parameters
- **Regional Deployment**: Deploy in your preferred Azure region for compliance and latency
- **Deployment Names**: Use your Azure deployment name as the model parameter, not OpenAI model names
- **Automatic Retries**: Built-in retry logic handles transient Azure service issues
